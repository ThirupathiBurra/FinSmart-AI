{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FinRAG Reference Implementation: Local Test\n",
                "\n",
                "This notebook demonstrates a **Minimum Viable Financial RAG (FinRAG)** system running entirely locally using:\n",
                "- **ChromaDB** (Local Vector Database)\n",
                "- **LangChain** (Orchestration)\n",
                "- **Shiva-k22/gemma-FinAI** (Finetuned Finance LLM)\n",
                "\n",
                "**Goal**: Demonstrate the full pipeline: Ingestion -> Chunking -> Vector Storage -> Retrieval -> Answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking dependencies...\n",
                        "chromadb is already installed.\n",
                        "Environment ready. FinRAG initialized.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Environment Setup & Auto-Install\n",
                "# Ensure dependencies are installed in the CURRENT Jupyter kernel.\n",
                "\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "print(\"Checking dependencies...\")\n",
                "try:\n",
                "    import chromadb\n",
                "    print(\"chromadb is already installed.\")\n",
                "except ImportError:\n",
                "    print(\"Installing chromadb...\")\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"chromadb\"])\n",
                "    import chromadb\n",
                "    print(\"chromadb installed successfully.\")\n",
                "\n",
                "import os\n",
                "import torch\n",
                "from langchain_community.vectorstores import Chroma\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Environment ready. FinRAG initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Shiva-k22/gemma-FinAI...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n",
                        "Device set to use mps\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 2: Load Finetuned Finance Model\n",
                "\n",
                "MODEL_NAME = \"Shiva-k22/gemma-FinAI\"\n",
                "print(f\"Loading {MODEL_NAME}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16 if torch.cuda.is_available() or torch.backends.mps.is_available() else torch.float32,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "pipe = pipeline(\n",
                "    \"text-generation\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    max_new_tokens=512,\n",
                "    do_sample=True,\n",
                "    temperature=0.1, \n",
                "    return_full_text=False\n",
                ")\n",
                "\n",
                "llm = HuggingFacePipeline(pipeline=pipe)\n",
                "print(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ingestion function defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Define Ingestion Pipeline\n",
                "\n",
                "def ingest_document(file_path):\n",
                "    print(f\"Ingesting {file_path}...\")\n",
                "    if not os.path.exists(file_path):\n",
                "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
                "    \n",
                "    # 1. Load\n",
                "    loader = PyPDFLoader(file_path)\n",
                "    pages = loader.load()\n",
                "    print(f\" -> Loaded {len(pages)} pages.\")\n",
                "    \n",
                "    # 2. Split (Financial Aware)\n",
                "    text_splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=1000,\n",
                "        chunk_overlap=100\n",
                "    )\n",
                "    chunks = text_splitter.split_documents(pages)\n",
                "    \n",
                "    # 3. Add Metadata\n",
                "    for i, chunk in enumerate(chunks):\n",
                "        chunk.metadata[\"chunk_id\"] = i\n",
                "        chunk.metadata[\"source\"] = os.path.basename(file_path)\n",
                "        if \"page\" not in chunk.metadata:\n",
                "            chunk.metadata[\"page\"] = \"Unknown\"\n",
                "            \n",
                "    print(f\" -> Created {len(chunks)} chunks with metadata.\")\n",
                "    return chunks\n",
                "\n",
                "print(\"Ingestion function defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embeddings ready. Vector Store waiting for data.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 4: Initialize Embeddings & Vector Store\n",
                "\n",
                "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
                "\n",
                "# Local Vector Store (ChromaDB)\n",
                "vectorstore = None \n",
                "print(\"Embeddings ready. Vector Store waiting for data.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ingesting /Users/burra/FinSmartAI/Annual-Report-2024-25.pdf...\n",
                        " -> Loaded 66 pages.\n",
                        " -> Created 233 chunks with metadata.\n",
                        "SUCCESS: Indexed 233 chunks from /Users/burra/FinSmartAI/Annual-Report-2024-25.pdf into Vector Store.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 5: Perform Ingestion (User Action)\n",
                "# Specify your file here.\n",
                "\n",
                "FILE_TO_INGEST = \"/Users/burra/FinSmartAI/Annual-Report-2024-25.pdf\"\n",
                "\n",
                "try:\n",
                "    # 1. Ingest chunks\n",
                "    chunks = ingest_document(FILE_TO_INGEST)\n",
                "    \n",
                "    # 2. Store in Chroma\n",
                "    if vectorstore is None:\n",
                "        vectorstore = Chroma.from_documents(\n",
                "            documents=chunks, \n",
                "            embedding=embedding_model,\n",
                "            collection_name=\"finrag_test\"\n",
                "        )\n",
                "    else:\n",
                "        vectorstore.add_documents(chunks)\n",
                "        \n",
                "    print(f\"SUCCESS: Indexed {len(chunks)} chunks from {FILE_TO_INGEST} into Vector Store.\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Ingestion skipped/failed: {e}\")\n",
                "    if \"not found\" in str(e).lower():\n",
                "         print(\"Please ensure the PDF file exists in this folder to proceed with real data.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieval Logic Ready.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 6: Retrieval Logic (FinRAG)\n",
                "\n",
                "def detect_intent(question):\n",
                "    question = question.lower()\n",
                "    if any(x in question for x in [\"summarize\", \"overview\", \"brief\", \"report\"]):\n",
                "        return \"SUMMARY\"\n",
                "    return \"SPECIFIC\"\n",
                "\n",
                "def get_context(question):\n",
                "    if vectorstore is None:\n",
                "        return \"\"\n",
                "        \n",
                "    intent = detect_intent(question)\n",
                "    k = 5 if intent == \"SUMMARY\" else 3\n",
                "    \n",
                "    docs = vectorstore.similarity_search(question, k=k)\n",
                "    \n",
                "    context_str = \"\"\n",
                "    for d in docs:\n",
                "        source = d.metadata.get('source', 'Unknown')\n",
                "        page = d.metadata.get('page', 'N/A')\n",
                "        context_str += f\"[Source: {source} | Page: {page}]\\n{d.page_content}\\n\\n\"\n",
                "    return context_str\n",
                "\n",
                "print(\"Retrieval Logic Ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Prompt Defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 7: FinRAG System Prompt\n",
                "\n",
                "from langchain.prompts import PromptTemplate\n",
                "\n",
                "FINRAG_PROMPT = PromptTemplate(\n",
                "    template=\"\"\"You are a senior financial analyst. Your role is to provide strict, factual answers based ONLY on the provided context.\n",
                "\n",
                "RULES:\n",
                "1. NO FLUFF: Do not use phrases like \"strong performance\" unless you cite numbers.\n",
                "2. GROUNDING: Every claim must use data from the context.\n",
                "3. CITATION: Mention [Source: Doc | Page: X] for every fact.\n",
                "4. IF UNCERTAIN: Say \"Information not available in the documents.\"\n",
                "\n",
                "CONTEXT:\n",
                "{context}\n",
                "\n",
                "QUESTION: \n",
                "{question}\n",
                "\n",
                "ANSWER:\"\"\",\n",
                "    input_variables=[\"context\", \"question\"]\n",
                ")\n",
                "\n",
                "print(\"Prompt Defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Q&A Function ready.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 8: Generation Function\n",
                "\n",
                "def answer_question(question):\n",
                "    context = get_context(question)\n",
                "    if not context.strip():\n",
                "         return \"[System] No documents ingested or no context found.\"\n",
                "         \n",
                "    prompt_text = FINRAG_PROMPT.format(context=context, question=question)\n",
                "    \n",
                "    print(f\"Processing: {question}...\")\n",
                "    response = llm.invoke(prompt_text)\n",
                "    return response\n",
                "\n",
                "print(\"Q&A Function ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing: Summarize the key financial highlights...\n",
                        "\n",
                        "--- Answer 1 ---\n",
                        "\n",
                        "Key financial highlights:\n",
                        "* Consolidated net profit of Rs. 7,796 crore in FY24, up 18% YoY.\n",
                        "* Gross NPAs declined to 6.1% from 7.3% in FY23.\n",
                        "* Net NPAs declined to 4.9% from 6.1% in FY23.\n",
                        "* Return on Equity (RoE) improved to 14.7% from 13.5% in FY23.\n",
                        "* Return on Assets (RoA) improved to 2.8% from 2.5% in FY23.\n",
                        "* Net interest margin (NIM) improved to 3.5% from 3.2% in FY23.\n",
                        "* Capital adequacy ratio improved to 15.2% from 14.8% in FY23.\n",
                        "* Dividend payout ratio increased to 40% from 30% in FY23.\n",
                        "* Total assets increased to Rs. 19,042 crore from Rs. 16,598 crore in FY23.\n",
                        "* Total deposits increased to Rs. 6,59,815 crore from Rs. 5,62,538 crore in FY23.\n",
                        "* Total loans and advances increased to Rs. 4,70,109 crore from Rs. 3,62,838 crore in FY23.\n",
                        "* Number of RRBs earning profit increased to 40 from 37 in FY23.\n",
                        "* Amount of profit earned by RRBs increased to Rs. 7,796 crore from Rs. 6,178 crore in FY23.\n",
                        "* Number of RRBs incurring losses decreased to 3 from 9 in FY23.\n",
                        "* Amount of losses incurred by RRBs decreased to Rs. 225 crore from Rs. 1,205 crore in FY23.\n",
                        "* Net profit of RRBs increased to Rs. 7,796 crore from Rs. 4,974 crore in FY23.\n",
                        "* GNPA (Amount) decreased to Rs. 28,913 crore from Rs. 33,190 crore in FY23.\n",
                        "* GNPA (%) decreased to 6.1% from 9.\n",
                        "Processing: What are the main risks mentioned?...\n",
                        "\n",
                        "--- Answer 2 ---\n",
                        "\n",
                        "The main risks mentioned are:\n",
                        "1. Credit Risk: The risk that borrowers will default on their loan repayments.\n",
                        "2. Market Risk: The risk that changes in market conditions will affect the value of investments.\n",
                        "3. Liquidity Risk: The risk that the company will not be able to meet its short-term obligations.\n",
                        "4. Operational Risk: The risk that the company will experience a disruption in its operations.\n",
                        "5. Regulatory Risk: The risk that changes in regulations will affect the company's profitability.\n",
                        "6. Reputational Risk: The risk that the company will suffer damage to its reputation.\n",
                        "7. Environmental Risk: The risk that the company will be affected by environmental events.\n",
                        "8. Political Risk: The risk that political events will affect the company's operations.\n",
                        "9. Geopolitical Risk: The risk that geopolitical events will affect the company's operations.\n",
                        "10. Natural Disaster Risk: The risk that natural disasters will affect the company's operations.\n",
                        "11. Cyber Risk: The risk that the company will be affected by cyberattacks.\n",
                        "12. Technological Risk: The risk that the company will be unable to keep up with technological advancements.\n",
                        "13. Legal Risk: The risk that the company will be sued for its actions.\n",
                        "14. Compliance Risk: The risk that the company will not comply with regulations.\n",
                        "15. Fraud Risk: The risk that the company will be the victim of fraud.\n",
                        "16. Liquidity Risk: The risk that the company will not be able to meet its short-term obligations.\n",
                        "17. Operational Risk: The risk that the company will experience a disruption in its operations.\n",
                        "18. Reputational Risk: The risk that the company will suffer damage to its reputation.\n",
                        "19. Environmental Risk: The risk that the company will be affected by environmental events.\n",
                        "20. Political Risk: The risk that political events will affect the company's operations.\n",
                        "21. Geopolitical Risk: The risk that geopolitical events will affect the company's operations.\n",
                        "22. Natural Disaster Risk: The risk that natural disasters will affect the company's operations.\n",
                        "23. Cyber Risk: The risk that the company will be affected by cyberattacks.\n",
                        "24. Technological Risk: The risk that the company will be unable to keep up with technological advancements.\n",
                        "25. Legal Risk: The risk that the company will be sued for its actions.\n",
                        "26. Compliance\n"
                    ]
                }
            ],
            "source": [
                "# Cell 9: Run Tests\n",
                "\n",
                "q1 = \"Summarize the key financial highlights\"\n",
                "ans1 = answer_question(q1)\n",
                "print(\"\\n--- Answer 1 ---\")\n",
                "print(ans1)\n",
                "\n",
                "q2 = \"What are the main risks mentioned?\"\n",
                "ans2 = answer_question(q2)\n",
                "print(\"\\n--- Answer 2 ---\")\n",
                "print(ans2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
